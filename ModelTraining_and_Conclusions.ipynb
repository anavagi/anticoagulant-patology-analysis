{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c03a399",
   "metadata": {},
   "source": [
    "¬°Perfecto! Te puedo dar un ejemplo b√°sico en Python usando **scikit-learn**, **XGBoost** y **CatBoost** para una variable continua, listo para usar en un notebook `.ipynb`. Supondr√© que tienes un dataset `X` (variables independientes) y `y` (variable continua).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Instalar librer√≠as si no las tienes\n",
    "!pip install scikit-learn xgboost catboost --quiet\n",
    "\n",
    "# Importar librer√≠as\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Modelos\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Simulamos un dataset de ejemplo\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=500, n_features=10, noise=0.2, random_state=42)\n",
    "\n",
    "# Dividir en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ **Decision Tree Regressor**\n",
    "\n",
    "```python\n",
    "# Crear modelo\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "# Entrenar\n",
    "dt_model.fit(X_train, y_train)\n",
    "# Predecir\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "# Evaluar\n",
    "print(\"Decision Tree R2:\", r2_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_dt)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Random Forest Regressor**\n",
    "\n",
    "```python\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest R2:\", r2_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **XGBoost Regressor**\n",
    "\n",
    "```python\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost R2:\", r2_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **CatBoost Regressor**\n",
    "\n",
    "```python\n",
    "# silent=True evita que imprima el progreso del entrenamiento\n",
    "cat_model = CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42, silent=True)\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "\n",
    "print(\"CatBoost R2:\", r2_score(y_test, y_pred_cat))\n",
    "print(\"CatBoost RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_cat)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Notas importantes:**\n",
    "\n",
    "* Ajusta los hiperpar√°metros (`n_estimators`, `learning_rate`, `max_depth`) seg√∫n tu dataset.\n",
    "* Para **interpretabilidad**, los √°rboles de decisi√≥n y Random Forest permiten usar `feature_importances_`.\n",
    "* XGBoost y CatBoost suelen **dar mejor desempe√±o** en datasets m√°s complejos.\n",
    "\n",
    "---\n",
    "\n",
    "Si quieres, puedo hacer una **versi√≥n resumida de este c√≥digo** que entrene los **3 modelos recomendados** y los compare autom√°ticamente en un solo bloque, lista para copiar y pegar en Jupyter. Esto hace todo m√°s limpio y r√°pido.\n",
    "\n",
    "¬øQuieres que haga eso?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
