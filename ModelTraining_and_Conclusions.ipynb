{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a47c85",
   "metadata": {},
   "source": [
    "# 3. Preparaci贸n de los datos\n",
    "\n",
    "## 3.1 Divisi贸n de los datos en conjuntos de datos de entrenamiento, validaci贸n y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c41dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/df_patients_observations_medications_INR.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fad699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the original dataframe so further manipulations will not affect it \n",
    "df_select = df.copy() \n",
    "categorical = df_select.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = df_select.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "numerical.remove('INR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82348bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325, 109, 109)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train + validation (= full) and test\n",
    "df_full_train, df_test = train_test_split(df_select, test_size=0.2, random_state=1)\n",
    "# now split the full into train and val, it should be the 20% of the 80%, which is 20/80=1/4=0.25\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1) \n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c0ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# separate the target\n",
    "y_train = df_train.INR.values\n",
    "y_val = df_val.INR.values\n",
    "\n",
    "# remove the target from the features\n",
    "del df_train['INR']\n",
    "del df_val['INR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4f56d",
   "metadata": {},
   "source": [
    "## 3.2 Codificaci贸n de las variables categ贸ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d814b0",
   "metadata": {},
   "source": [
    "Usamos Scikit-Learn DictVectorizer para codificar caracter铆sticas categ贸ricas (toma un diccionario y lo convierte en un vector, es decir, en un numpy.array). Es un m茅todo de los llamados One-Hot Encoding (OHE) para convertir las caracter铆sticas categ贸ricas (no afectar铆a las num茅ricas) en columnas con valores binarios, con tantas columnas como valores tome la variable categ贸rica. Como veremos a continuaci贸n, esta codificaci贸n proporciona un buen rendimiento del modelo de referencia inicial ('baseline model'), por lo que no exploramos los efectos de otros algoritmos de codificaci贸n, como la funci贸n get_dummies() o la librer铆a Category Encoders, por ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cac549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer \n",
    "\n",
    "dv = DictVectorizer(sparse=False) # False bcs is not a sparse matrix (we do not have many zeros)\n",
    "\n",
    "# TRAIN\n",
    "train_dict = df_train[categorical].to_dict(orient='records') # records = to do it row-wise, not col-wise\n",
    "X_train_cat = dv.fit_transform(train_dict) # make it a vector\n",
    "\n",
    "# VAL\n",
    "val_dict = df_val[categorical].to_dict(orient='records')\n",
    "X_val_cat = dv.transform(val_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f531a",
   "metadata": {},
   "source": [
    "Ahora bien, nuestro dataframe de las 'features' categ贸ricas tiene informaci贸n redundante. Por ejemplo, la columna de la caracterist铆ca 'ExcerciseAngina' que pod铆a tener valores 'N' o 'Y' ahora di贸 lugar a dos columnas, una llamada 'ExcerciseAngina=N' y otra 'ExcerciseAngina=Y' que claramente no son independientes y est谩n totalmente correlacionadas, miremos por ejemplo, el conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6a832",
   "metadata": {},
   "source": [
    "**LAS CORRELACIONES SON LAS MISMAS QUE ANTERIORMENTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81765c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert de data array to a dataframe\n",
    "df_X_train_cat = pd.DataFrame(X_train_cat,columns= dv.get_feature_names_out())\n",
    "X_train_cat = df_X_train_cat.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fac39b",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con el conjunto de validaci贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc658c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data array to a dataframe\n",
    "df_X_val_cat = pd.DataFrame(X_val_cat,columns= dv.get_feature_names_out())\n",
    "\n",
    "# remove redundant columns\n",
    "df_X_val_cat.drop(['ExerciseAngina=N', 'FastingBS=H', 'Sex=F', 'ST_Slope=Up'], \n",
    "                    axis=1, inplace=True)\n",
    "\n",
    "# convert the dataframe to a np.array again\n",
    "X_val_cat = df_X_val_cat.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd92edc",
   "metadata": {},
   "source": [
    "y los nombres de las columnas con 'features' categoricas que ha sido codificadas son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c050a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_update = df_X_train_cat.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37382e4",
   "metadata": {},
   "source": [
    "## 3.3 Scaling de las variables num茅ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6541bf2",
   "metadata": {},
   "source": [
    "Usamos Scikit-Learn StandardScaler para acotar cada caracter铆stica num茅rica sustrayendo su valor medio y dividiendo el resultado entre su desviaci贸n est谩ndar de manera que al final los valores de esa columna se encuentren entre 0 y 1. Si no aplicamos esta acotaci贸n, las columnas con valores en un rango m谩s alto tendr铆an m谩s representaci贸n y varios modelos como la regresi贸n log铆stica no convergen. Otros modelos, como los basados en 谩rboles, no son sensibles a la diferencia entre los rangos de las caracter铆sticas y no es un necesario aplicar este procedimiento. En ingl茅s este procedimiento se conoce como \"scaling\", y a veces se traduce como \"normalizaci贸n\" porque al final todas las caracter铆sticas siguen la misma \"norma\" o \"rango\", pero como tantas otras veces, aqu铆 los nombres pueden dar lugar a confusi贸n (ver otra definici贸n de \"normalizaci贸n\" por ejemplo en esta referencia, donde el resultado final es una distribuci贸n gaussiana para cada columna). Como veremos a continuaci贸n, acotar los datos proporciona un rendimiento razonable para el modelo de referencia inicial, por lo tanto, no exploramos los efectos de otros algoritmos de estandarizaci贸n, como MixMaxScaler, por ejemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e847840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# TRAIN\n",
    "X_train_num = df_train[numerical].values\n",
    "X_train_num = scaler.fit_transform(X_train_num)\n",
    "\n",
    "# VAL\n",
    "X_val_num = df_val[numerical].values\n",
    "X_val_num = scaler.transform(X_val_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c9afe",
   "metadata": {},
   "source": [
    "Unimos las matrices num茅ricas y categ贸ricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "X_train = np.column_stack([X_train_num, X_train_cat])\n",
    "\n",
    "# VAL\n",
    "X_val = np.column_stack([X_val_num, X_val_cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b2529",
   "metadata": {},
   "source": [
    "# 4. Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2b2cc",
   "metadata": {},
   "source": [
    "En esta divisi贸n aleatoria, y en todas las ocasiones siguientes en las que usemos un generador de n煤meros aleatorios, fijamos la semilla del generador de n煤meros aleatorios para asegurar la reproducibilidad de los resultados. Fijamos el random_state = 1 pero podr铆a ser 2 o cualquier otro valor, no afectar谩 significativamente a las prediciones, pero con esta correcci贸n siempre obtendremos exactamente los mismos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03a399",
   "metadata": {},
   "source": [
    "隆Perfecto! Te puedo dar un ejemplo b谩sico en Python usando **scikit-learn**, **XGBoost** y **CatBoost** para una variable continua, listo para usar en un notebook `.ipynb`. Supondr茅 que tienes un dataset `X` (variables independientes) y `y` (variable continua).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Instalar librer铆as si no las tienes\n",
    "!pip install scikit-learn xgboost catboost --quiet\n",
    "\n",
    "# Importar librer铆as\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Modelos\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Simulamos un dataset de ejemplo\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=500, n_features=10, noise=0.2, random_state=42)\n",
    "\n",
    "# Dividir en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1锔 **Decision Tree Regressor**\n",
    "\n",
    "```python\n",
    "# Crear modelo\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "# Entrenar\n",
    "dt_model.fit(X_train, y_train)\n",
    "# Predecir\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "# Evaluar\n",
    "print(\"Decision Tree R2:\", r2_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_dt)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2锔 **Random Forest Regressor**\n",
    "\n",
    "```python\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest R2:\", r2_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3锔 **XGBoost Regressor**\n",
    "\n",
    "```python\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost R2:\", r2_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4锔 **CatBoost Regressor**\n",
    "\n",
    "```python\n",
    "# silent=True evita que imprima el progreso del entrenamiento\n",
    "cat_model = CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42, silent=True)\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "\n",
    "print(\"CatBoost R2:\", r2_score(y_test, y_pred_cat))\n",
    "print(\"CatBoost RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_cat)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    " **Notas importantes:**\n",
    "\n",
    "* Ajusta los hiperpar谩metros (`n_estimators`, `learning_rate`, `max_depth`) seg煤n tu dataset.\n",
    "* Para **interpretabilidad**, los 谩rboles de decisi贸n y Random Forest permiten usar `feature_importances_`.\n",
    "* XGBoost y CatBoost suelen **dar mejor desempe帽o** en datasets m谩s complejos.\n",
    "\n",
    "---\n",
    "\n",
    "Si quieres, puedo hacer una **versi贸n resumida de este c贸digo** que entrene los **3 modelos recomendados** y los compare autom谩ticamente en un solo bloque, lista para copiar y pegar en Jupyter. Esto hace todo m谩s limpio y r谩pido.\n",
    "\n",
    "驴Quieres que haga eso?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
