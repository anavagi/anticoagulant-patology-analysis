{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269efe84",
   "metadata": {},
   "source": [
    "![banner](./images/banner.png \"banner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756558f",
   "metadata": {},
   "source": [
    "# <font color=#6290C3>Modelo de aprendizaje automático para la predicción del Ratio Internacional Normalizado (INR) en pacientes bajo terapia con Antagonistas de la Vitamina K </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d5bc0",
   "metadata": {},
   "source": [
    "<a href=#pre>Requisitos previos</a><br>\n",
    "1. <a href=#project-description>Descripción del proyecto</a><br>\n",
    "    1.1 <a href=#project-description-goal>Objetivo</a><br>\n",
    "    1.2 <a href=#project-description-data>Data</a><br>\n",
    "    1.3 <a href=#project-description-software>Software</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc181cf",
   "metadata": {},
   "source": [
    "# 1. Descripción del proyecto\n",
    "\n",
    "Este proyecto consiste en una predición de datos ....\n",
    "\n",
    "- Es una tarea supervisada, es decir, el modelo se ha de entrar con muchos datos. Una vez entrenado el modelo, lo aplicamos a una nueva fila donde falta un valor, aquello que queremos predecir, y si el entrenamiento fue bueno, el modelo podrá predecir ese dato que falta. Las columnas para las que sí conocemos siempre todos los valores se llaman \"características\" o \"features\" en inglés.\n",
    "\n",
    "- El dato que les falta a esas nuevas filas es el correspondiente a la columna 'target' (también llamado \"destino\" \"objetivo\" o \"outcome\") y es lo que intentamos predecir, en este caso, es la clase a la que pertenece cada fila.\n",
    "\n",
    "- La columna 'target' tiene valores continuos, por lo que usaremos modelos de regresión.\n",
    "\n",
    "- Los datos de nuestra columna \"target\" puede tomar distintos valores entre los rangos 0.1 y 3.0\n",
    "\n",
    "- Si los datos no hubieran sido etiquetados (es decir, las clases no estuvieran definidas desde el inicio), necesitaríamos un modelo de aprendizaje automático no supervisado (es decir, el modelo debe encontrar los grupos o 'clusters' en inglés).\n",
    "\n",
    "Ajustaremos los modelos de regresión de aprendizaje automático más populares a unos datos de pacientes con anticoagulantes y seleccionaremos el modelo con mejor rendimiento. Para verificar el rendimiento del modelo, separamos aleatoriamente el conjunto de datos en 3 subconjuntos:\n",
    "\n",
    "- Un subconjunto de datos para entrenar el modelo: representan los datos 'antiguos', donde no faltan valores, es decir, todos los valores de todas las columnas incluida la 'target' son conocidos, así sabemos si eses pacientes sufrieron o no fallo cardíaco,\n",
    "\n",
    "- Un subconjunto de los datos para validar el modelo: representan datos 'nuevos' porque reservamos los valores de la columna 'target', los separamos y los guardamos, es decir, pretendemos que no existen, como si fueran nueves pacientes, le damos ese subconjunto de validación al modelo ya entrenado y comparamos los resultados del modelo, es decir, sus prediciones, con los valores reales que teníamos reservados. Como ajustaremos los parámetros para hacer que los resultados del modelo se acerquen cada vez más a los valores reales, el modelo verá este conjunto de datos de validación con mucha frecuencia, lo que puede hacer que el modelo se desvíe, muestre preferencia o sesgo ('bias' en inglés) hacia el conjunto de datos de validación (explicamos la definición de sesgo en los siguientes Jupyter Notebooks sobre Explicabilidad XAI.ipynb y Cuantificación de Incertidumbre UQ.ipynb en este mismo repositorio), es decir, puede estar sobreajustado ('overfitted') y no generalizar bien cuando lo aplicamos a otros datos, por eso separamos un último grupo:\n",
    "\n",
    "- Un subconjunto de los datos para probar los resultados del modelo: para la verificación final que compara los resultados del modelo ajustado con datos que el modelo entrenado y validado nunca ha visto antes; es la mejor manera que tenemos de simular nuevos datos reales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ccfc5",
   "metadata": {},
   "source": [
    "## 1.1 Objetivo\n",
    "\n",
    "El objetivo es predecir el INR de un paciente con anticoagulantes mediante varaibles analiticas y ritmo de vida... y otras características consideradas factores de riesgo. Las conclusiones de este análisis pueden ayudar en la detección temprana y la prevención de diagnosticos complicados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8172883",
   "metadata": {},
   "source": [
    "## 1.2 Datos\n",
    "\n",
    "El conjunto de datos que usamos para entrenar nuestro modelo son datos sinteticos de Synthnea, un software opensource de estados unidos. Este programa generará mediante sus comandos historias completas de pacientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d94c25",
   "metadata": {},
   "source": [
    "Para ejecutar el programa ejecutremos `java -jar synthea.jar` este procesará los valores que se indiquen en el archivo de configuración **synthea.properties** indicaremos lo siguiente:\n",
    "\n",
    "- default_population = 5.000 -> cantida de pacientes\n",
    "- exporter.csv.export = true  -> se exportaran los datos a ficheros csv\n",
    "- exporter.csv.append_mode = true   -> si se ejecuta otra vez el archivo se actualizaran los datos del csv\n",
    "- generate.modules = cardiovascular_disease, atrial_fibrillation, venous_thromboembolism  -> permite priorizar la generación de pacientes con estas patologias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a012d29",
   "metadata": {},
   "source": [
    "Synthea export data as CSV into `./output/csv`.  Las historias de pacientes se reparten en los siguientes csv.\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| [`allergies.csv`](#allergies) | Patient allergy data. |\n",
    "| [`careplans.csv`](#careplans) | Patient care plan data, including goals. |\n",
    "| [`claims.csv`](#claims) | Patient claim data. |\n",
    "| [`claims_transactions.csv`](#claims-transactions) | Transactions per line item per claim. |\n",
    "| [`conditions.csv`](#conditions) | Patient conditions or diagnoses. |\n",
    "| [`devices.csv`](#devices) | Patient-affixed permanent and semi-permanent devices. |\n",
    "| [`encounters.csv`](#encounters) | Patient encounter data. |\n",
    "| [`imaging_studies.csv`](#imaging-studies) | Patient imaging metadata. |\n",
    "| [`immunizations.csv`](#immunizations) | Patient immunization data. |\n",
    "| [`medications.csv`](#medications) | Patient medication data. |\n",
    "| [`observations.csv`](#observations) | Patient observations including vital signs and lab reports. |\n",
    "| [`organizations.csv`](#organizations) | Provider organizations including hospitals. |\n",
    "| [`patients.csv`](#patients) | Patient demographic data. |\n",
    "| [`payer_transitions.csv`](#payer-transitions) | Payer Transition data (i.e. changes in health insurance). |\n",
    "| [`payers.csv`](#payers) | Payer organization data. |\n",
    "| [`procedures.csv`](#procedures) | Patient procedure data including surgeries. |\n",
    "| [`providers.csv`](#providers) | Clinicians that provide patient care. |\n",
    "| [`supplies.csv`](#supplies) | Supplies used in the provision of care. |\n",
    "\n",
    "\n",
    "Para nuestro analisis solo utilizaremos:\n",
    "\n",
    "# Observations\n",
    "| | Column Name | Data Type | Required? | Description |\n",
    "|-|-------------|-----------|-----------|-------------|\n",
    "| | Date | iso8601 UTC Date (`yyyy-MM-dd'T'HH:mm'Z'`) | `true` | The date and time the observation was performed. |\n",
    "| :old_key: | Patient | UUID | `true` | Foreign key to the Patient. |\n",
    "| :old_key: | Encounter | UUID | `true` | Foreign key to the Encounter where the observation was performed. |\n",
    "| | Category | String | `false` | Observation category. |\n",
    "| | Code | String | `true` | Observation or Lab code from LOINC |\n",
    "| | Description | String | `true` | Description of the observation or lab. |\n",
    "| | Value | String | `true` | The recorded value of the observation. Often numeric, but some values can be verbose, for example, multiple-choice questionnaire responses. |\n",
    "| | Units | String | `false` | The units of measure for the value. |\n",
    "| | Type | String | `true` | The datatype of `Value`: `text` or `numeric` |\n",
    "\n",
    "\n",
    "# Patients\n",
    "| | Column Name | Data Type | Required? | Description |\n",
    "|-|-------------|-----------|-----------|-------------|\n",
    "| :key: | Id | UUID | `true` | Primary Key. Unique Identifier of the patient. |\n",
    "| | BirthDate | Date (`YYYY-MM-DD`) | `true` | The date the patient was born. |\n",
    "| | DeathDate | Date (`YYYY-MM-DD`) | `false` | The date the patient died. |\n",
    "| | SSN | String | `true` | Patient Social Security identifier. |\n",
    "| | Drivers | String | `false` | Patient Drivers License identifier. |\n",
    "| | Passport | String | `false` | Patient Passport identifier. |\n",
    "| | Prefix | String | `false` | Name prefix, such as `Mr.`, `Mrs.`, `Dr.`, etc. |\n",
    "| | First | String | `true` | First name of the patient. |\n",
    "| | Middle | String | `false` | Middle name of the patient. |\n",
    "| | Last | String | `true` | Last or surname of the patient. |\n",
    "| | Suffix | String | `false` | Name suffix, such as `PhD`, `MD`, `JD`, etc. |\n",
    "| | Maiden | String | `false` | Maiden name of the patient. |\n",
    "| | Marital | String | `false` | Marital Status. `M` is married, `S` is single. Currently no support for divorce (`D`) or widowing (`W`) |\n",
    "| | Race | String | `true` | Description of the patient's primary race. |\n",
    "| | Ethnicity | String | `true` | Description of the patient's primary ethnicity. |\n",
    "| | Gender | String | `true` | Gender. `M` is male, `F` is female. |\n",
    "| | BirthPlace | String | `true` | Name of the town where the patient was born. |\n",
    "| | Address | String | `true` | Patient's street address without commas or newlines. |\n",
    "| | City | String | `true` | Patient's address city. |\n",
    "| | State | String | `true` | Patient's address state. |\n",
    "| | County | String | `false` | Patient's address county. |\n",
    "| | FIPS County Code | String | `false` | Patient's FIPS county code. |\n",
    "| | Zip | String | `false` | Patient's zip code. |\n",
    "| | Lat | Numeric | `false` | Latitude of Patient's address. |\n",
    "| | Lon | Numeric | `false` | Longitude of Patient's address. |\n",
    "| | Healthcare_Expenses | Numeric | `true` | The total lifetime cost of healthcare to the patient (i.e. what the patient paid). |\n",
    "| | Healthcare_Coverage | Numeric | `true` | The total lifetime cost of healthcare services that were covered by Payers (i.e. what the insurance company paid). |\n",
    "| | Income | Numeric | `true` | Annual income for the Patient |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21b3fb",
   "metadata": {},
   "source": [
    "El variable 'target', es decir, lo que queremos predecir, se llama \"INR\", y es una variable continua que toma las siguientes interpretaciones:\n",
    "\n",
    "- <2.0: sí, es decir, pacientes que sí presentan riesgo de padecer una cardiopatía,\n",
    "- 2.0 y 3.0: no, es decir, pacientes que no presentan riesgo de padecer una cardiopatía.\n",
    "- 3.0: no, es decir, pacientes que no presentan riesgo de padecer una cardiopatía."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967a514",
   "metadata": {},
   "source": [
    "## 1.3 Software\n",
    "\n",
    "Importamos las siguientes librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ac103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data exploration and preparation  \n",
    "#from sklearn.metrics import mutual_info_score, roc_auc_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction import DictVectorizer \n",
    "#from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# machine learning models\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# model evaluation\n",
    "#from sklearn.metrics import accuracy_score, f1_score, auc, recall_score, precision_score, confusion_matrix\n",
    "#from sklearn.metrics import make_scorer\n",
    "#from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "\n",
    "# plotting and displaying in the notebook\n",
    "#import seaborn as sns\n",
    "#from matplotlib import pyplot as plt\n",
    "#from IPython.display import display\n",
    "#from sklearn import tree\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd401094",
   "metadata": {},
   "source": [
    "# 2. Exploración y visualización de datos\n",
    "\n",
    "## 2.1 Carga de datos\n",
    "\n",
    "Vamos a cargar los datos del fichero `patients.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = pd.read_csv(\"output/csv/patients.csv\")\n",
    "df_patients.head()\n",
    "#df_patients.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d23e3e",
   "metadata": {},
   "source": [
    "El conjunto de datos contiene información de **5,826 pacientes y 28 variables que describen sus características básicas y demográficas**, como edad, género, etnia, estado civil, lugar de nacimiento e ingresos. Incluye además datos administrativos (por ejemplo, número de seguro social o pasaporte) y médicos (como cobertura de salud). En general, ofrece una visión completa del perfil socio-demográfico de cada paciente.\n",
    "\n",
    "Vamos a cargar los datos del fichero `observations.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ce858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observations = pd.read_csv(\"output/csv/observations.csv\")\n",
    "df_observations.head()\n",
    "#df_observations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097ca83",
   "metadata": {},
   "source": [
    "Este conjunto de datos es considerablemente amplio, con **4,627,376 registros y 9 variables** que describen distintas interacciones clínicas de los pacientes. Contiene información sobre la fecha, tipo de encuentro, categoría médica, código y descripción de cada evento, así como los valores y unidades asociados. Representa un registro detallado de las actividades y mediciones realizadas en el contexto sanitario.\n",
    "\n",
    "Dentro de este conjunto se identifica nuestra **variable objetivo 'target', que indica si el paciente cuenta con un registro de INR** y el valor correspondiente. Esta variable permite evaluar la presencia y magnitud de dicha medición, siendo la clave de este análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda24b5",
   "metadata": {},
   "source": [
    "## 2.2 Generación del csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23046c2b",
   "metadata": {},
   "source": [
    "Se seleccionan únicamente los registros de pacientes no difuntos y aquellos que contienen controles de INR, ya que el análisis se centra en esa región. Posteriormente, se realiza la limpieza de datos sobre el subconjunto resultante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6c4f4",
   "metadata": {},
   "source": [
    "**Pacientes difuntos**  \n",
    "Los datos cuentan con una columna de fecha de difunción, eliminaremos aquellos registros que tengan DeathDate diferente a nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a62696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cantidad de pacientes vivos:\")\n",
    "print(df_patients['DEATHDATE'].isna().sum())\n",
    "\n",
    "print(\"Cantidad de pacientes difuntos:\")\n",
    "print(df_patients['DEATHDATE'].notna().sum())\n",
    "\n",
    "#df_patients = df_patients[df_patients['DEATHDATE'].isna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895d85f",
   "metadata": {},
   "source": [
    "Combinamos los valores para ello necesitamos saber los pacientes que toman anticoagulantes y sus observaciones, filtraremos por las observaciones cuya descripción contenga INR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cantidad de filas que contienen 'INR' en la descripción:\")\n",
    "print(df_observations['DESCRIPTION'].str.contains('INR', case=False, na=False).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c51fce",
   "metadata": {},
   "source": [
    "Posteriormente, combinaremos los pacientes existentes con los pacientes de las observaciones mediante el Id del paciente. De esta manera, podremos realizar nuestros analisis a partir de un solo fichero de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que:\n",
    "# - df_patients (izquierda) tiene la columna clave 'Id'\n",
    "# - df_observations (derecha) tiene la columna clave 'PATIENT'\n",
    "\n",
    "df_patients_observations = pd.merge(\n",
    "    df_patients,        # DataFrame de la izquierda\n",
    "    df_observations,  # DataFrame de la derecha\n",
    "    left_on='Id',           # <--- Columna clave en el DataFrame de la izquierda (df_demograficos)\n",
    "    right_on='PATIENT',     # <--- Columna clave en el DataFrame de la derecha (df_observations_pivot)\n",
    "    how='left'              # Tipo de unión (mantiene todos los pacientes del df_demograficos)\n",
    ")\n",
    "#Eliminamos la columna PATIENT para no tener duplicados\n",
    "\n",
    "columnas_a_eliminar = ['PATIENT']\n",
    "df_patients_observations = df_patients_observations.drop(columns=columnas_a_eliminar)\n",
    "df_patients_observations.to_csv('patients_observations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1c35e",
   "metadata": {},
   "source": [
    "### Simplificación de las variables y pivotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6b7e5",
   "metadata": {},
   "source": [
    ".py para los nombres de las variables y pivotación el csv de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77429055",
   "metadata": {},
   "source": [
    "Los modelos de aprendizaje automático son más precisos cuanto más datos usamos para entrenarlos y este conjunto de datos puede ser demasiado pequeño para conseguir un gran rendimiento. Comprobemos si es así.\n",
    "\n",
    "En las siguientes subsecciones limpiamos y preparamos los datos porque queremos entrenar el modelo con la mejor información. Los datos pueden estar etiquetados incorrectamente, presentar errores tipográficos, .... Además, es posible que los modelos no acepten valores faltantes (como 'null' o NaN) o ciertos tipos de datos o puede que no convergan si los datos no están escaleados. Finalmente, queremos que los modelos separen la información relevante que las personas no podemos inferir, por lo tanto, podemos usar la información que sí inferimos simplemente (como corregir errores tipográficos ...) para ayudarlos (y ahorrar tiempo y energía)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5166e54",
   "metadata": {},
   "source": [
    "## 2.3 Limpieza de datos\n",
    "\n",
    "### Valores faltantes y data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58882076",
   "metadata": {},
   "source": [
    "No se detectan valores faltantes en el conjunto de datos. Eso significa que no están representados por 'NaN' en el conjunto de datos, pero a menudo se usan otros valores para la información faltante como 'desconocido', 'unknown', '?', '9999', ... y vamos estar atentes a esto.\n",
    "\n",
    "Vamos a reducir este dataset a simplemente las variables AGE convertida a raiz del BIRTHDATE, y GENDER. Posteriormente, vamos a añadir más valores para trabajar sobre un solo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients['BIRTHDATE'] = pd.to_datetime(df_patients['BIRTHDATE'], errors='coerce')\n",
    "\n",
    "df_patients['AGE'] = ((pd.Timestamp('today') - df_patients['BIRTHDATE']).dt.days / 365.25).astype(int)\n",
    "df_patients['AGE'] = df_patients['AGE'].fillna(df_patients['AGE'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4249de0",
   "metadata": {},
   "source": [
    "Posteriormente nos quedaremos con las 3 variables clave: Id, AGE y GENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients[['Id','AGE', 'GENDER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in different lists the names of the categorical and numerical columns since they will be treated different \n",
    "categorical = df_patients.select_dtypes(include=['object']).columns.tolist()  # for strings \n",
    "numerical = df_patients.select_dtypes(include=['int64','float64']).columns.tolist() # for numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d66f3",
   "metadata": {},
   "source": [
    "### Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.duplicated().count() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b6c15",
   "metadata": {},
   "source": [
    "El método duplicated() da 'False' si la fila no está duplicada y el método count() solo cuenta los valores 'True', ya que hay 918 valores verdaderos en 918 filas, no hay filas duplicadas (en la página de Kaggle del conjunto de datos se dice que los duplicados ya se eliminaron)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5444ca",
   "metadata": {},
   "source": [
    "### Rangos y estadística básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39389bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the stats of numerical features\n",
    "df_patients.describe(include = np.number).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b889d45",
   "metadata": {},
   "source": [
    "### Valores únicos\n",
    "\n",
    "El método describe() ya mostraba los valores únicos para las variables categóricas. Comprobamos ahora los valores únicos de las características numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5d135",
   "metadata": {},
   "source": [
    "# 2.4 Feature importance y analisis de la variable 'target'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
